{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Y68HaoYtlp83psybPCOQIs3jxba-UtcA","timestamp":1712037980114}],"authorship_tag":"ABX9TyNXD/4vGyE/vzrxwwhNovWv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bFIxttyOVgXE"},"outputs":[],"source":["def normalize(train, test = None):\n","  scaler = preprocessing.StandardScaler()\n","  scaled_train = scaler.fit_transform(train)\n","  if test is None:\n","    return scaled_train\n","  else:\n","    scaled_test = scaler.transform(test)\n","    return scaled_train, scaled_test"]},{"cell_type":"markdown","source":["### De data trecuta"],"metadata":{"id":"dLLEJ4FPWS7Y"}},{"cell_type":"code","source":["word_vectors = []\n","\n","for word in bow_model.cuv[1:]:\n","    word_vector = np.zeros(len(bow_model.cuv))\n","    word_vector[bow_model.vocabulary[word]] = 1\n","    word_vectors.append(word_vector)\n","\n","word_vectors = np.array(word_vectors)\n","values = svm_model.decision_function(word_vectors)\n","sorted_indexes = np.argsort(values)\n","\n","poz = sorted_indexes[:10]\n","neg = sorted_indexes[-10:]\n","\n","print(\"the first 10 negative words are\")\n","for i in neg:\n","    print(bow_model.cuv[i] + ',')\n","\n","print(\"\\nthe first 10 positive words are\")\n","for i in poz:\n","    print(bow_model.cuv[i] + ',')"],"metadata":{"id":"Ys-SiqHFWR98"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3-fold\n","from sklearn import utils\n","training_data, training_labels = utils.shuffle(training_data, training_labels, random_state = 0)\n","\n","from sklearn import preprocessing\n","def normalize(train, test = None):\n","  scaler = preprocessing.StandardScaler()\n","  scaled_train = scaler.fit_transform(train)\n","  if test is None:\n","    return scaled_train\n","  else:\n","    scaled_test = scaler.transform(test)\n","    return scaled_train, scaled_test\n","\n","k = len(training_data) // 3\n","training_data_1, training_labels_1 = training_data[:k], training_labels[:k]\n","training_data_2, training_labels_2 = training_data[k:2*k], training_labels[k:2*k]\n","training_data_3, training_labels_3 = training_data[2*k:], training_labels[2*k:]\n","\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","model = LinearRegression()\n","#1\n","training_sample_1, testing_sample_1 = normalize(np.concatenate((training_data_1, training_data_2)), training_data_3)\n","model.fit(training_sample_1, np.concatenate((training_labels_1, training_labels_2)))\n","mean_absolute_error1 = mean_absolute_error(training_labels_3, model.predict(testing_sample_1))\n","mean_squared_error1 = mean_squared_error(training_labels_3, model.predict(testing_sample_1))\n","\n","#2\n","training_sample_2, testing_sample_2 = normalize(np.concatenate((training_data_1, training_data_3)), training_data_2)\n","model.fit(training_sample_2, np.concatenate((training_labels_1, training_labels_3)))\n","mean_absolute_error2 = mean_absolute_error(training_labels_2, model.predict(testing_sample_2))\n","mean_squared_error2 = mean_squared_error(training_labels_2, model.predict(testing_sample_2))\n","\n","\n","#3\n","training_sample_3, testing_sample_3 = normalize(np.concatenate((training_data_2, training_data_3)), training_data_1)\n","model.fit(training_sample_3, np.concatenate((training_labels_2, training_labels_3)))\n","mean_absolute_error3 = mean_absolute_error(training_labels_1, model.predict(testing_sample_3))\n","mean_squared_error3 = mean_squared_error(training_labels_1, model.predict(testing_sample_3))\n","\n","print(f\"Mean MAE is {(mean_absolute_error1 + mean_absolute_error2 + mean_absolute_error3)/3}\")\n","print(f\"Mean MSE is {(mean_squared_error1 + mean_squared_error2 + mean_squared_error3)/3}\")\n"],"metadata":{"id":"H2pRE3bDYQZ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for indice in range(0, 4):\n","  alpha = 10**indice\n","  model = Ridge(alpha)\n","  #1\n","  training_sample_1, testing_sample_1 = normalize(np.concatenate((training_data_1, training_data_2)), training_data_3)\n","  model.fit(training_sample_1, np.concatenate((training_labels_1, training_labels_2)))\n","  mean_absolute_error1 = mean_absolute_error(training_labels_3, model.predict(testing_sample_1))\n","  mean_squared_error1 = mean_squared_error(training_labels_3, model.predict(testing_sample_1))\n","\n","  #2\n","  training_sample_2, testing_sample_2 = normalize(np.concatenate((training_data_1, training_data_3)), training_data_2)\n","  model.fit(training_sample_2, np.concatenate((training_labels_1, training_labels_3)))\n","  mean_absolute_error2 = mean_absolute_error(training_labels_2, model.predict(testing_sample_2))\n","  mean_squared_error2 = mean_squared_error(training_labels_2, model.predict(testing_sample_2))\n","\n","\n","  #3\n","  training_sample_3, testing_sample_3 = normalize(np.concatenate((training_data_3, training_data_2)), training_data_1)\n","  model.fit(training_sample_3, np.concatenate((training_labels_3, training_labels_2)))\n","  mean_absolute_error3 = mean_absolute_error(training_labels_1, model.predict(testing_sample_3))\n","  mean_squared_error3 = mean_squared_error(training_labels_1, model.predict(testing_sample_3))\n","\n","  print(f\"\\n {indice + 1}.Output for alpha = {alpha}\\n\")\n","  print(f\"Mean MAE = {(mean_absolute_error1 + mean_absolute_error2 + mean_absolute_error3)/3}\")\n","  print(f\"Mean MSE = {(mean_squared_error1 + mean_squared_error2 + mean_squared_error3)/3}\")\n"],"metadata":{"id":"uXTbVr-HZ6ob"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Ridge(1000)\n","training_samples = normalize(training_data)\n","model.fit(training_samples, training_labels)\n","print(f\"Coefficients are {model.coef_}\\n\")\n","print(f\"Bias is {model.intercept_}\\n\")\n","print(f\"Most significant feature is {np.argsort(np.abs(model.coef_))[-1]}\\n\")\n","print(f\"Second most significant feature is {np.argsort(np.abs(model.coef_))[-2]}\\n\")\n","print(f\"Least significant feature is {np.argsort(np.abs(model.coef_))[0]}\\n\")"],"metadata":{"id":"ZmP-0WpLeSYx"},"execution_count":null,"outputs":[]}]}